{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cdd8f59-90bb-4b39-b407-240c2cad92bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000, Loss: 0.249444\n",
      "Epoch 200/10000, Loss: 0.248367\n",
      "Epoch 300/10000, Loss: 0.244862\n",
      "Epoch 400/10000, Loss: 0.233645\n",
      "Epoch 500/10000, Loss: 0.207283\n",
      "Epoch 600/10000, Loss: 0.176251\n",
      "Epoch 700/10000, Loss: 0.156527\n",
      "Epoch 800/10000, Loss: 0.145982\n",
      "Epoch 900/10000, Loss: 0.140130\n",
      "Epoch 1000/10000, Loss: 0.136603\n",
      "Epoch 1100/10000, Loss: 0.134307\n",
      "Epoch 1200/10000, Loss: 0.132718\n",
      "Epoch 1300/10000, Loss: 0.131564\n",
      "Epoch 1400/10000, Loss: 0.130692\n",
      "Epoch 1500/10000, Loss: 0.130014\n",
      "Epoch 1600/10000, Loss: 0.129473\n",
      "Epoch 1700/10000, Loss: 0.129032\n",
      "Epoch 1800/10000, Loss: 0.128667\n",
      "Epoch 1900/10000, Loss: 0.128360\n",
      "Epoch 2000/10000, Loss: 0.128098\n",
      "Epoch 2100/10000, Loss: 0.127872\n",
      "Epoch 2200/10000, Loss: 0.127676\n",
      "Epoch 2300/10000, Loss: 0.127504\n",
      "Epoch 2400/10000, Loss: 0.127352\n",
      "Epoch 2500/10000, Loss: 0.127216\n",
      "Epoch 2600/10000, Loss: 0.127095\n",
      "Epoch 2700/10000, Loss: 0.126986\n",
      "Epoch 2800/10000, Loss: 0.126888\n",
      "Epoch 2900/10000, Loss: 0.126798\n",
      "Epoch 3000/10000, Loss: 0.126716\n",
      "Epoch 3100/10000, Loss: 0.126641\n",
      "Epoch 3200/10000, Loss: 0.126572\n",
      "Epoch 3300/10000, Loss: 0.126509\n",
      "Epoch 3400/10000, Loss: 0.126450\n",
      "Epoch 3500/10000, Loss: 0.126396\n",
      "Epoch 3600/10000, Loss: 0.126345\n",
      "Epoch 3700/10000, Loss: 0.126298\n",
      "Epoch 3800/10000, Loss: 0.126254\n",
      "Epoch 3900/10000, Loss: 0.126213\n",
      "Epoch 4000/10000, Loss: 0.126174\n",
      "Epoch 4100/10000, Loss: 0.126138\n",
      "Epoch 4200/10000, Loss: 0.126103\n",
      "Epoch 4300/10000, Loss: 0.126071\n",
      "Epoch 4400/10000, Loss: 0.126041\n",
      "Epoch 4500/10000, Loss: 0.126012\n",
      "Epoch 4600/10000, Loss: 0.125984\n",
      "Epoch 4700/10000, Loss: 0.125958\n",
      "Epoch 4800/10000, Loss: 0.125934\n",
      "Epoch 4900/10000, Loss: 0.125910\n",
      "Epoch 5000/10000, Loss: 0.125888\n",
      "Epoch 5100/10000, Loss: 0.125867\n",
      "Epoch 5200/10000, Loss: 0.125846\n",
      "Epoch 5300/10000, Loss: 0.125827\n",
      "Epoch 5400/10000, Loss: 0.125808\n",
      "Epoch 5500/10000, Loss: 0.125790\n",
      "Epoch 5600/10000, Loss: 0.125773\n",
      "Epoch 5700/10000, Loss: 0.125757\n",
      "Epoch 5800/10000, Loss: 0.125741\n",
      "Epoch 5900/10000, Loss: 0.125726\n",
      "Epoch 6000/10000, Loss: 0.125712\n",
      "Epoch 6100/10000, Loss: 0.125698\n",
      "Epoch 6200/10000, Loss: 0.125685\n",
      "Epoch 6300/10000, Loss: 0.125672\n",
      "Epoch 6400/10000, Loss: 0.125659\n",
      "Epoch 6500/10000, Loss: 0.125647\n",
      "Epoch 6600/10000, Loss: 0.125636\n",
      "Epoch 6700/10000, Loss: 0.125624\n",
      "Epoch 6800/10000, Loss: 0.125614\n",
      "Epoch 6900/10000, Loss: 0.125603\n",
      "Epoch 7000/10000, Loss: 0.125593\n",
      "Epoch 7100/10000, Loss: 0.125583\n",
      "Epoch 7200/10000, Loss: 0.125574\n",
      "Epoch 7300/10000, Loss: 0.125565\n",
      "Epoch 7400/10000, Loss: 0.125556\n",
      "Epoch 7500/10000, Loss: 0.125547\n",
      "Epoch 7600/10000, Loss: 0.125539\n",
      "Epoch 7700/10000, Loss: 0.125531\n",
      "Epoch 7800/10000, Loss: 0.125523\n",
      "Epoch 7900/10000, Loss: 0.125515\n",
      "Epoch 8000/10000, Loss: 0.125508\n",
      "Epoch 8100/10000, Loss: 0.125501\n",
      "Epoch 8200/10000, Loss: 0.125494\n",
      "Epoch 8300/10000, Loss: 0.125487\n",
      "Epoch 8400/10000, Loss: 0.125480\n",
      "Epoch 8500/10000, Loss: 0.125474\n",
      "Epoch 8600/10000, Loss: 0.125467\n",
      "Epoch 8700/10000, Loss: 0.125461\n",
      "Epoch 8800/10000, Loss: 0.125455\n",
      "Epoch 8900/10000, Loss: 0.125449\n",
      "Epoch 9000/10000, Loss: 0.125444\n",
      "Epoch 9100/10000, Loss: 0.125438\n",
      "Epoch 9200/10000, Loss: 0.125433\n",
      "Epoch 9300/10000, Loss: 0.125427\n",
      "Epoch 9400/10000, Loss: 0.125422\n",
      "Epoch 9500/10000, Loss: 0.125417\n",
      "Epoch 9600/10000, Loss: 0.125412\n",
      "Epoch 9700/10000, Loss: 0.125407\n",
      "Epoch 9800/10000, Loss: 0.125403\n",
      "Epoch 9900/10000, Loss: 0.125398\n",
      "Epoch 10000/10000, Loss: 0.125393\n",
      "\n",
      "Predicted Output after training:\n",
      "[[0.98467332]\n",
      " [0.01730437]\n",
      " [0.50040857]\n",
      " [0.49936986]]\n",
      "Gayatri Kulkarni -53004230002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid Activation Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the Sigmoid Function for backpropagation\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# ANN class to simulate feedforward and backpropagation\n",
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.5):\n",
    "        # Initialize weights randomly\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        \n",
    "        # Initialize biases randomly\n",
    "        self.bias_hidden = np.random.rand(1, hidden_size)\n",
    "        self.bias_output = np.random.rand(1, output_size)\n",
    "        \n",
    "        # Set the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    # Feedforward process\n",
    "    def feedforward(self, X):\n",
    "        # Hidden layer activation\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Output layer activation\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output = sigmoid(self.output_input)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    # Backpropagation process\n",
    "    def backpropagation(self, X, y):\n",
    "        # Error at the output layer\n",
    "        output_error = y - self.output\n",
    "        output_delta = output_error * sigmoid_derivative(self.output)\n",
    "        \n",
    "        # Error at the hidden layer\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update the weights and biases using the deltas\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * self.learning_rate\n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta) * self.learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "    \n",
    "    # Train the neural network\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Feedforward\n",
    "            self.feedforward(X)\n",
    "            # Backpropagation\n",
    "            self.backpropagation(X, y)\n",
    "            # Print loss every 100 epochs\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                loss = np.mean(np.square(y - self.output))\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input dataset (XNOR problem)\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    \n",
    "    # Output dataset (XNOR output)\n",
    "    y = np.array([[1],\n",
    "                  [0],\n",
    "                  [0],\n",
    "                  [1]])\n",
    "    \n",
    "    # Parameters\n",
    "    input_size = X.shape[1]  # 2 features in input\n",
    "    hidden_size = 2          # 2 neurons in hidden layer\n",
    "    output_size = 1          # 1 output neuron (binary classification)\n",
    "    \n",
    "    # Create the neural network\n",
    "    ann = ArtificialNeuralNetwork(input_size, hidden_size, output_size, learning_rate=0.5)\n",
    "    \n",
    "    # Train the neural network\n",
    "    ann.train(X, y, epochs=10000)\n",
    "    \n",
    "    # Test the neural network\n",
    "    output = ann.feedforward(X)\n",
    "    print(\"\\nPredicted Output after training:\")\n",
    "    print(output)\n",
    "    print(\"Gayatri Kulkarni -53004230002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e10b191-dead-43d9-b3bb-55c165074dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output after training:\n",
      "[[0.76339362]\n",
      " [0.29147892]\n",
      " [0.29152897]\n",
      " [0.62217968]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function (activation function) and its derivative for backpropagation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# XNOR Dataset (inputs and outputs)\n",
    "# XNOR truth table\n",
    "# A  B  |  Output\n",
    "# 0  0  |    1\n",
    "# 0  1  |    0\n",
    "# 1  0  |    0\n",
    "# 1  1  |    1\n",
    "\n",
    "inputs = np.array([[0, 0], \n",
    "                   [0, 1], \n",
    "                   [1, 0], \n",
    "                   [1, 1]])\n",
    "\n",
    "# Expected outputs for XNOR gate\n",
    "outputs = np.array([[1], [0], [0], [1]])\n",
    "\n",
    "# Seed random numbers to make the output deterministic\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize weights randomly with mean 0\n",
    "input_layer_neurons = 2  # Two input neurons (A and B)\n",
    "hidden_layer_neurons = 2  # Two neurons in hidden layer\n",
    "output_neurons = 1  # One output neuron\n",
    "\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
    "weights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training process with backpropagation\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # Feedforward\n",
    "    hidden_layer_input = np.dot(inputs, weights_input_hidden)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Backpropagation\n",
    "    error = outputs - predicted_output\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "\n",
    "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Updating weights\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    weights_input_hidden += inputs.T.dot(d_hidden_layer) * learning_rate\n",
    "\n",
    "# Test the network after training\n",
    "print(\"Predicted Output after training:\")\n",
    "print(predicted_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46dd756d-0db7-469f-ad41-43bc43bc92a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000, Loss: 0.250292\n",
      "Epoch 200/10000, Loss: 0.250075\n",
      "Epoch 300/10000, Loss: 0.249924\n",
      "Epoch 400/10000, Loss: 0.249774\n",
      "Epoch 500/10000, Loss: 0.249568\n",
      "Epoch 600/10000, Loss: 0.249206\n",
      "Epoch 700/10000, Loss: 0.248409\n",
      "Epoch 800/10000, Loss: 0.246312\n",
      "Epoch 900/10000, Loss: 0.240445\n",
      "Epoch 1000/10000, Loss: 0.225935\n",
      "Epoch 1100/10000, Loss: 0.203495\n",
      "Epoch 1200/10000, Loss: 0.185304\n",
      "Epoch 1300/10000, Loss: 0.170710\n",
      "Epoch 1400/10000, Loss: 0.145633\n",
      "Epoch 1500/10000, Loss: 0.088836\n",
      "Epoch 1600/10000, Loss: 0.041711\n",
      "Epoch 1700/10000, Loss: 0.022656\n",
      "Epoch 1800/10000, Loss: 0.014583\n",
      "Epoch 1900/10000, Loss: 0.010465\n",
      "Epoch 2000/10000, Loss: 0.008050\n",
      "Epoch 2100/10000, Loss: 0.006488\n",
      "Epoch 2200/10000, Loss: 0.005406\n",
      "Epoch 2300/10000, Loss: 0.004618\n",
      "Epoch 2400/10000, Loss: 0.004020\n",
      "Epoch 2500/10000, Loss: 0.003552\n",
      "Epoch 2600/10000, Loss: 0.003178\n",
      "Epoch 2700/10000, Loss: 0.002872\n",
      "Epoch 2800/10000, Loss: 0.002617\n",
      "Epoch 2900/10000, Loss: 0.002402\n",
      "Epoch 3000/10000, Loss: 0.002218\n",
      "Epoch 3100/10000, Loss: 0.002060\n",
      "Epoch 3200/10000, Loss: 0.001921\n",
      "Epoch 3300/10000, Loss: 0.001800\n",
      "Epoch 3400/10000, Loss: 0.001692\n",
      "Epoch 3500/10000, Loss: 0.001596\n",
      "Epoch 3600/10000, Loss: 0.001510\n",
      "Epoch 3700/10000, Loss: 0.001433\n",
      "Epoch 3800/10000, Loss: 0.001362\n",
      "Epoch 3900/10000, Loss: 0.001298\n",
      "Epoch 4000/10000, Loss: 0.001240\n",
      "Epoch 4100/10000, Loss: 0.001187\n",
      "Epoch 4200/10000, Loss: 0.001137\n",
      "Epoch 4300/10000, Loss: 0.001092\n",
      "Epoch 4400/10000, Loss: 0.001050\n",
      "Epoch 4500/10000, Loss: 0.001011\n",
      "Epoch 4600/10000, Loss: 0.000975\n",
      "Epoch 4700/10000, Loss: 0.000941\n",
      "Epoch 4800/10000, Loss: 0.000909\n",
      "Epoch 4900/10000, Loss: 0.000879\n",
      "Epoch 5000/10000, Loss: 0.000852\n",
      "Epoch 5100/10000, Loss: 0.000825\n",
      "Epoch 5200/10000, Loss: 0.000801\n",
      "Epoch 5300/10000, Loss: 0.000777\n",
      "Epoch 5400/10000, Loss: 0.000755\n",
      "Epoch 5500/10000, Loss: 0.000735\n",
      "Epoch 5600/10000, Loss: 0.000715\n",
      "Epoch 5700/10000, Loss: 0.000696\n",
      "Epoch 5800/10000, Loss: 0.000678\n",
      "Epoch 5900/10000, Loss: 0.000661\n",
      "Epoch 6000/10000, Loss: 0.000645\n",
      "Epoch 6100/10000, Loss: 0.000630\n",
      "Epoch 6200/10000, Loss: 0.000615\n",
      "Epoch 6300/10000, Loss: 0.000601\n",
      "Epoch 6400/10000, Loss: 0.000588\n",
      "Epoch 6500/10000, Loss: 0.000575\n",
      "Epoch 6600/10000, Loss: 0.000563\n",
      "Epoch 6700/10000, Loss: 0.000551\n",
      "Epoch 6800/10000, Loss: 0.000539\n",
      "Epoch 6900/10000, Loss: 0.000528\n",
      "Epoch 7000/10000, Loss: 0.000518\n",
      "Epoch 7100/10000, Loss: 0.000508\n",
      "Epoch 7200/10000, Loss: 0.000498\n",
      "Epoch 7300/10000, Loss: 0.000489\n",
      "Epoch 7400/10000, Loss: 0.000480\n",
      "Epoch 7500/10000, Loss: 0.000471\n",
      "Epoch 7600/10000, Loss: 0.000463\n",
      "Epoch 7700/10000, Loss: 0.000455\n",
      "Epoch 7800/10000, Loss: 0.000447\n",
      "Epoch 7900/10000, Loss: 0.000439\n",
      "Epoch 8000/10000, Loss: 0.000432\n",
      "Epoch 8100/10000, Loss: 0.000425\n",
      "Epoch 8200/10000, Loss: 0.000418\n",
      "Epoch 8300/10000, Loss: 0.000411\n",
      "Epoch 8400/10000, Loss: 0.000405\n",
      "Epoch 8500/10000, Loss: 0.000399\n",
      "Epoch 8600/10000, Loss: 0.000393\n",
      "Epoch 8700/10000, Loss: 0.000387\n",
      "Epoch 8800/10000, Loss: 0.000381\n",
      "Epoch 8900/10000, Loss: 0.000375\n",
      "Epoch 9000/10000, Loss: 0.000370\n",
      "Epoch 9100/10000, Loss: 0.000365\n",
      "Epoch 9200/10000, Loss: 0.000360\n",
      "Epoch 9300/10000, Loss: 0.000355\n",
      "Epoch 9400/10000, Loss: 0.000350\n",
      "Epoch 9500/10000, Loss: 0.000345\n",
      "Epoch 9600/10000, Loss: 0.000341\n",
      "Epoch 9700/10000, Loss: 0.000336\n",
      "Epoch 9800/10000, Loss: 0.000332\n",
      "Epoch 9900/10000, Loss: 0.000327\n",
      "Epoch 10000/10000, Loss: 0.000323\n",
      "\n",
      "Predicted Output after training:\n",
      "[[0.01978687]\n",
      " [0.9828782 ]\n",
      " [0.98289096]\n",
      " [0.01777236]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid Activation Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the Sigmoid Function for backpropagation\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# ANN class to simulate feedforward and backpropagation\n",
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.5):\n",
    "        # Initialize weights randomly\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        \n",
    "        # Initialize biases randomly\n",
    "        self.bias_hidden = np.random.rand(1, hidden_size)\n",
    "        self.bias_output = np.random.rand(1, output_size)\n",
    "        \n",
    "        # Set the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    # Feedforward process\n",
    "    def feedforward(self, X):\n",
    "        # Hidden layer activation\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Output layer activation\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output = sigmoid(self.output_input)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    # Backpropagation process\n",
    "    def backpropagation(self, X, y):\n",
    "        # Error at the output layer\n",
    "        output_error = y - self.output\n",
    "        output_delta = output_error * sigmoid_derivative(self.output)\n",
    "        \n",
    "        # Error at the hidden layer\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update the weights and biases using the deltas\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * self.learning_rate\n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta) * self.learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "    \n",
    "    # Train the neural network\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Feedforward\n",
    "            self.feedforward(X)\n",
    "            # Backpropagation\n",
    "            self.backpropagation(X, y)\n",
    "            # Print loss every 100 epochs\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                loss = np.mean(np.square(y - self.output))\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input dataset (XOR problem)\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    \n",
    "    # Output dataset (XOR output)\n",
    "    y = np.array([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]])\n",
    "    \n",
    "    # Parameters\n",
    "    input_size = X.shape[1]  # 2 features in input\n",
    "    hidden_size = 2          # 2 neurons in hidden layer\n",
    "    output_size = 1          # 1 output neuron (binary classification)\n",
    "    \n",
    "    # Create the neural network\n",
    "    ann = ArtificialNeuralNetwork(input_size, hidden_size, output_size, learning_rate=0.5)\n",
    "    \n",
    "    # Train the neural network\n",
    "    ann.train(X, y, epochs=10000)\n",
    "    \n",
    "    # Test the neural network\n",
    "    output = ann.feedforward(X)\n",
    "    print(\"\\nPredicted Output after training:\")\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd4a9c-ed11-470c-b5ba-e03efa06bcce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
